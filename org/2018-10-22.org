* Bitmarkd Peer

  Following paragraphs are described based on ~v8.2~ commit ~b013d5e~.

** Initialization

   Flow starts at ~bitmarkd/main.go#278: peer.Initialise~.

   peer package contains ~peerData~ with read/write lock (~sync.RWMutex~),
   rpc listeners (~lstn~), rpc clients (~conn~), etc.

   #+BEGIN_SRC go
     type peerData struct {
         sync.RWMutex // to allow locking

         log *logger.L // logger

         lstn listener  // for RPC responses
         conn connector // for RPC requests

         connectorClients []*upstream.Upstream

         publicKey []byte

         clientCount int
         blockHeight uint64

         // for background
         background *background.T

         // set once during initialise
         initialised bool
     }

     type listener struct {
         log         *logger.L
         chain       string
         version     string      // server version
         push        *zmq.Socket // signal send
         pull        *zmq.Socket // signal receive
         socket4     *zmq.Socket // IPv4 traffic
         socket6     *zmq.Socket // IPv6 traffic
         monitor4    *zmq.Socket // IPv4 socket monitor
         monitor6    *zmq.Socket // IPv6 socket monitor
         connections uint64      // total incoming connections
     }
   #+END_SRC

   In ~peer/setup.go: Initialise~, it reads private/public key file,
   registers to ~announce~ which will broadcast events (~setAnnounce~), initialize
   objects to process request/response.

*** RPC Response Listener

    ~listener~ is an object includes logger, ipv4/ipv6 sockets,
    receiver/sender, ipv4/ipv6 monitors. Detail data can be found as
    below:

    #+BEGIN_SRC go
      type listener struct {
          log         *logger.L
          chain       string
          version     string      // server version
          push        *zmq.Socket // signal send
          pull        *zmq.Socket // signal receive
          socket4     *zmq.Socket // IPv4 traffic
          socket6     *zmq.Socket // IPv6 traffic
          monitor4    *zmq.Socket // IPv4 socket monitor
          monitor6    *zmq.Socket // IPv6 socket monitor
          connections uint64      // total incoming connections
      }
    #+END_SRC

    ~listener~ is initialized at ~peer/setup.go#109: globalData.lstn.initialise~.

    Initialization process as below:
**** creates logger

**** creates sender (push) and receiver (pull) for each socket

     At ~peer/listener.go#71: zmqutil.NewSignalPair(listenerSignal)~.

     Be aware that ~listenerSignal =
     "inproc://bitmark-listener-signal"~ is defined by zeroMQ, ~inproc~
     can be referenced [[http://api.zeromq.org/4-1:zmq-bind#toc2][here]], it denotes local in-process
     (inter-thread) communication transport.

**** allocates ipv4/ipv6 sockets

     At ~peer/listener.go#77: zmqutil.NewBind~

**** sets monitors of ipv4/ipv6 each

     At ~peer/listener.go#84: zmqutil.NewMonitor~.

*** RPC Request Connector

    ~connector~ is an object includes logger, client lists, connection
    state, other node's largest block height, etc.

    connector state is an integer value to represents finite state
    machine state.

    #+BEGIN_SRC go
      type connector struct {
          log *logger.L

          preferIPv6 bool

          staticClients []*upstream.Upstream

          dynamicClients list.List

          state connectorState

          theClient        *upstream.Upstream // client used for fetching blocks
          startBlockNumber uint64             // block number where local chain forks
          height           uint64             // block number on best node
          samples          int                // counter to detect missed block broadcast
      }

      type Upstream struct {
          sync.RWMutex
          log         *logger.L
          client      *zmqutil.Client
          registered  bool
          blockHeight uint64
          shutdown    chan<- struct{}
      }
    #+END_SRC

    It is initialized at ~peer/connector.go#65: initialise~.

    Initialization process as below:

**** create logger
**** parse ip address and port

     At ~peer/connector.go#87: util.NewConnection~

**** create upstream client

     Inside ~peer/connector.go#107: upstream.New~ creates another go
     routine for function ~upstreamRunner~ which will wait for incoming messages.

**** connect to server

     At ~peer/connector.go#117: client.Connect~

*** Start to process incoming/outgoing messages

    It is started by go routine at ~peer/setup.go#128:
    background.Start~. With in this function, it invokes each object's
    method of ~Run~.

    #+BEGIN_SRC go
      go func(p Process, shutdown <-chan struct{}, finished chan<- struct{}) {
                  // pass the shutdown to the Run loop for shutdown signalling
                  p.Run(args, shutdown)
                  // flag for the stop routine to wait for shutdown
                  close(finished)
              }(p, shutdown, finished)
    #+END_SRC

**** Listener

     ~listener~ ~Run~ method is defined at ~peer/listner.go#101: Run~, for different
     incoming message/event invokes different function:

     #+BEGIN_SRC go
      for {
          sockets, _ := poller.Poll(-1)
          for _, socket := range sockets {
              switch s := socket.Socket; s {
              case lstn.socket4:
                  lstn.process(lstn.socket4)
              case lstn.socket6:
                  lstn.process(lstn.socket6)
              case lstn.pull:
                  s.RecvMessageBytes(0)
                  break loop
              case lstn.monitor4:
                  lstn.handleEvent(lstn.monitor4)
              case lstn.monitor6:
                  lstn.handleEvent(lstn.monitor6)
              }
          }
      }
     #+END_SRC

     The format of ipv4/ipv6 incoming message as follows:

     The format of receive message bytes as follows:

     The format of event as follows:

**** Connector

     ~connector~ ~Run~ method is defined at ~peer/connector.go#182: Run~, for
     different outgoing message invokes different function:

     #+BEGIN_SRC go
       for {
           // wait for shutdown
           log.Debug("waitingâ€¦")

           select {
           case <-shutdown:
               break loop
           case item := <-queue:
               c, _ := util.PackedConnection(item.Parameters[1]).Unpack()
               conn.log.Debugf("received control: %s  public key: %x  connect: %x %q", item.Command, item.Parameters[0], item.Parameters[1], c)
               //connectToUpstream(conn.log, conn.clients, conn.dynamicStart, item.Command, item.Parameters[0], item.Parameters[1])
               conn.connectUpstream(item.Command, item.Parameters[0], item.Parameters[1])

           case <-time.After(cycleInterval):
               conn.process()
           }
       }
     #+END_SRC

** Format of message

   Typicall command will be as follow, parameters are append at last
   of message with each 8 bytes long

  | Command | Chain Mode | Parameters   |
  |---------+------------+--------------|
  | 1 byte  | 8 bytes    | 8 bytes each |

  e.g. When register to another node, the command will be sent as
  follows:

  #+BEGIN_SRC
    0x52 => "R"
    0x74657374696e67 => "testing"
    9f5f6122d09c18bef1c9b96e773cf0b784198b70e4c3becbe4951d642ee4484c => parameters depend by each command
  #+END_SRC

** Process Listener message

   It is defined at ~peer/listener.go#157: process~. When receiving every
   peer message, checks data validation. After decode message sent by
   zeroMQ, an array of strings will be returned.

   First string in array will be chain type.

   #+BEGIN_SRC go
     theChain := string(data[0])
   #+END_SRC

   Second string in array will be operation action, third string in arary
   will be parameters (if exists).

   #+BEGIN_SRC go
     fn := string(data[1])
     parameters := data[2:]
   #+END_SRC

*** Server information ("I")

    Returns server information with following format.

    #+BEGIN_SRC go
      serverInfo{
          Version: lstn.version,
          Chain:   mode.ChainName(),
          Normal:  mode.Is(mode.Normal),
          Height:  block.GetHeight(),
      }
      result, err = json.Marshal(info)
    #+END_SRC

    ~result~ is converted into json format.

*** Get block number ("N")

    Returns block height.

    #+BEGIN_SRC go
      blockNumber := block.GetHeight()
      result = make([]byte, 8)
      binary.BigEndian.PutUint64(result, blockNumber)
    #+END_SRC

    ~result~ is format into big-endian 64 bits.

*** Get packed block ("B")

    Returns block number specified by parameter. Return error if first parameter
    length is not 8 bytes (64 bits).

    #+BEGIN_SRC go
      if 1 != len(parameters) {
          err = fault.ErrMissingParameters
      } else if 8 == len(parameters[0]) {
          result = storage.Pool.Blocks.Get(parameters[0])
          if nil == result {
              err = fault.ErrBlockNotFound
          }
      } else {
          err = fault.ErrBlockNotFound
      }
    #+END_SRC

*** Get block hash ("H")

    Return block hash specified by parameters. Return error if first
    parameter length is not 8 bytes (64 bits).

    #+BEGIN_SRC go
      if 1 != len(parameters) {
          err = fault.ErrMissingParameters
      } else if 8 == len(parameters[0]) {
          number := binary.BigEndian.Uint64(parameters[0])
          d, e := block.DigestForBlock(number)
          if nil == e {
              result = d[:]
          } else {
              err = e
          }
      } else {
          err = fault.ErrBlockNotFound
      }
    #+END_SRC

*** Register to another node ("R")

    In order to register as new peer, some information are necessary
    to provide including chain type, public key, listener ip/port,
    timestamp. Before continuing the flow , all data will be checked
    if it's valid or not.

    #+BEGIN_SRC go
      var binTs [8]byte
      binary.BigEndian.PutUint64(binTs[:], uint64(ts.Unix()))

      _, err = socket.Send(fn, zmq.SNDMORE)
      logger.PanicIfError("Listener", err)
      _, err = socket.Send(chain, zmq.SNDMORE)
      logger.PanicIfError("Listener", err)
      _, err = socket.SendBytes(publicKey, zmq.SNDMORE)
      logger.PanicIfError("Listener", err)
      _, err = socket.SendBytes(listeners, zmq.SNDMORE)
      logger.PanicIfError("Listener", err)
      _, err = socket.SendBytes(binTs[:], 0)
      logger.PanicIfError("Listener", err)
    #+END_SRC

*** Default action

    Default operation is to process received data. The function is
    defined at ~peer/process.go#23: processSubscription~.

    Different data type will be passed, zero or more parameters may be
    transfered.

    #+BEGIN_SRC go
      func processSubscription(log *logger.L, command string, arguments [][]byte) {

          dataLength := len(arguments)
          switch string(command) { ... }
          ...
      }
    #+END_SRC

    Exact ~command~ listed as follows:

**** Block ("block")

     Send block information in parameter.

     #+BEGIN_SRC go
       if dataLength < 1 {
           log.Warnf("block with too few data: %d items", dataLength)
           return
       }
       log.Infof("received block: %x", arguments[0])
       if !mode.Is(mode.Normal) {
           err := fault.ErrNotAvailableDuringSynchronise
           log.Warnf("failed assets: error: %s", err)
       } else {
           messagebus.Bus.Blockstore.Send("remote", arguments[0])
       }
     #+END_SRC

**** Asset ("assets")

     Unpack message and cache it if message is an asset record. Detail
     asset information is unpacked by ~peer/process.go#129:
     processAssets~.

     #+BEGIN_SRC go
       transaction, n, err := transactionrecord.Packed(packed).Unpack(mode.IsTesting())
       ...
       switch tx := transaction.(type) {
       case *transactionrecord.AssetData:
           _, packedAsset, err := asset.Cache(tx)
           if nil != err {
               return err
           }
           if nil != packedAsset {
               ok = true
           }
     #+END_SRC

     Incoming asset record is cached at ~asset/asset.go#81: Cache~. The reason asset is
     cached because asset record should always comes with an issue, so asset
     cannot come alone.

     Asset record will be checked if already existed in cache pool.

     #+BEGIN_SRC go
       switch tx := transaction.(type) {
       case *transactionrecord.AssetData:
           if tx.Name == asset.Name &&
               tx.Fingerprint == asset.Fingerprint &&
               tx.Metadata == asset.Metadata &&
               tx.Registrant.String() == asset.Registrant.String() {

               r.state = pendingState // extend timeout
               packedAsset = nil      // already seen
           } else {
               dataWouldChange = true
           }
       }
     #+END_SRC

     After that, asset record will be put into a queue.

     #+BEGIN_SRC go
       globalData.expiry.queue <- assetId
     #+END_SRC

     ~asset~ object is initialized at ~asset/asset.go#53: Initialise~,
     which will invoke a background job to process the queue just put:

     #+BEGIN_SRC go
       globalData.background = background.Start(processes, globalData.log)
     #+END_SRC

     The background job will call ~Run~ located at ~asset/expiry.go#22:
     Run~. It setup default timeout to 72 hours at
     ~constants/constants.go~. An asset record is cached for 72 hours
     for user to pay fee for miner.

**** Issue ("issues")

     Incoming issue command is processed by ~peer/process.go#169:
     processIssues~. After some data checkings, extract all issues
     inside parameter.

     #+BEGIN_SRC go
       issues := make([]*transactionrecord.BitmarkIssue, 0, 100)
       for 0 != len(packedIssues) {
           transaction, n, err := packedIssues.Unpack(mode.IsTesting())
           if nil != err {
               return err
           }

           switch tx := transaction.(type) {
           case *transactionrecord.BitmarkIssue:
               issues = append(issues, tx)
               issueCount += 1
           default:
               return fault.ErrTransactionIsNotAnIssue
           }
           packedIssues = packedIssues[n:]
       }
     #+END_SRC

     Actual issue processing logic is at ~reservoir/issues.go#48: StoreIssues~.

***** Verify issue

      Make sure issues waiting for process has not exceed limits of
      ~maximumIssues~ (100).

      For all issues, check if asset record already exist because issue
      must go with asset record.

      #+BEGIN_SRC go
       packedIssue, err := issue.Pack(issue.Owner)
       if nil != err {
           return nil, false, err
       }

       if !asset.Exists(issue.AssetId) {
           return nil, false, fault.ErrAssetNotFound
       }
      #+END_SRC

      ~asset.Exists~ check asset info from both confirmed/cached list.

      After checking, create ~txId~.

      #+BEGIN_SRC go
       txId := packedIssue.MakeLink()
      #+END_SRC

      From database table, issue record is placed inside ~transaction~
      table, that's why naming of cache pool is ~UnverifiedTxIndex~.

      If an issue is alread existed in the ~UnverfiedTxIndex~ pool, mark
      it as duplicate and do other checking.

      #+BEGIN_SRC go
        if _, ok := cache.Pool.UnverifiedTxIndex.Get(txId.String()); ok {
            // if duplicate, activate pay id check
            duplicate = true
        }
      #+END_SRC

      If an issue is already verifed, return error because it shouldn't happen.

      #+BEGIN_SRC go
        if _, ok := cache.Pool.VerifiedTx.Get(txId.String()); ok {
            return nil, false, fault.ErrTransactionAlreadyExists
        }
      #+END_SRC

      If an issue is already confirmed, return error because it
      shouldn't happen.

      #+BEGIN_SRC go
        if storage.Pool.Transactions.Has(txId[:]) {
            return nil, false, fault.ErrTransactionAlreadyExists
        }
      #+END_SRC

***** Compute payment info

      Generate payid, nonce, etc. Payment id is generated at
      ~pay/payid.go#20: NewPayId~. Nonce is generated at
      ~reservoir/paynonce.go#23: NewPayNonce~. Difficulty is generated
      at ~reservoir/difficulty.go#35: ScaledDifficulty~.

      #+BEGIN_SRC go
        payId := pay.NewPayId(separated)
        nonce := NewPayNonce()
        difficulty := ScaledDifficulty(count)

        result := &IssueInfo{
            Id:         payId,
            Nonce:      nonce,
            Difficulty: difficulty,
            TxIds:      txIds,
            Packed:     bytes.Join(separated, []byte{}),
            Payments:   nil,
        }
      #+END_SRC

      If payment id is already generated, do nothing.

      #+BEGIN_SRC go
        if _, ok := cache.Pool.UnverifiedTxEntries.Get(payId.String()); ok {
            globalData.log.Debugf("duplicate pay id: %s", payId)
            return result, true, nil
        }
      #+END_SRC

***** Check for duplicated issue

      If a duplicate issue is detected but cannot found any duplicated
      payment info, return error.

      #+BEGIN_SRC go
        if duplicate {
            globalData.log.Debugf("overlapping pay id: %s", payId)
            return nil, false, fault.ErrTransactionAlreadyExists
        }
      #+END_SRC

***** Determine payment & block number for the issue

      If an issue record passed all previous checking, then it's time
      to find the block where asset record being placed. ~GetNB~ stands for
      get block number (NB).

      #+BEGIN_SRC go
        assetBlockNumber := uint64(0)
        scan_for_one_asset:
        for _, issue := range issues {
            bn, t := storage.Pool.Assets.GetNB(issue.AssetId[:])
            if nil == t || 0 == bn {
                assetBlockNumber = 0     // cannot determine a single payment block
                break scan_for_one_asset // because of unconfirmed asset
            } else if 0 == assetBlockNumber {
                assetBlockNumber = bn // block number of asset
            } else if assetBlockNumber != bn {
                assetBlockNumber = 0     // cannot determin a single payment block
                break scan_for_one_asset // because of multiple assets
            }
        }
      #+END_SRC

      Get payment record from block.

      #+BEGIN_SRC go
        if assetBlockNumber > genesis.BlockNumber { // avoid genesis block

            blockNumberKey := make([]byte, 8)
            binary.BigEndian.PutUint64(blockNumberKey, assetBlockNumber)

            p := getPayment(blockNumberKey)
            if nil == p { // would be an internal database error
                globalData.log.Errorf("missing payment for asset id: %s", issues[0].AssetId)
                return nil, false, fault.ErrAssetNotFound
            }

            result.Payments = make([]transactionrecord.PaymentAlternative, 0, len(p))
            // multiply fees for each currency
            for _, r := range p {
                total := r.Amount * uint64(len(txIds))
                pa := transactionrecord.PaymentAlternative{
                    &transactionrecord.Payment{
                        Currency: r.Currency,
                        Address:  r.Address,
                        Amount:   total,
                    },
                }
                result.Payments = append(result.Payments, pa)
            }
        }
      #+END_SRC

***** Store issue into pool

      Generate issue data.

      #+BEGIN_SRC go
        entry := &unverifiedItem{
            itemData: &itemData{
                txIds:        txIds,
                links:        nil,
                assetIds:     uniqueAssetIds,
                transactions: separated,
                nonce:        nil,
            },
            //nonce:      nonce, // ***** FIX THIS: this value seems not used
            difficulty: difficulty,
            payments:   result.Payments,
        }
      #+END_SRC

      If payment for an issue is already recieved, store payment and
      remove data from ~OrphanPayment~.

      #+BEGIN_SRC go
        if val, ok := cache.Pool.OrphanPayment.Get(payId.String()); ok {
            detail := val.(*PaymentDetail)

            if acceptablePayment(detail, result.Payments) {

                for i, txId := range txIds {
                    cache.Pool.VerifiedTx.Put(
                        txId.String(),
                        &verifiedItem{
                            itemData:    entry.itemData,
                            transaction: separated[i],
                            index:       i,
                        },
                    )
                }
                cache.Pool.OrphanPayment.Delete(payId.String())
                return result, false, nil
            }
        }
      #+END_SRC

      If no payment found, put issues into cache pool.

      #+BEGIN_SRC go
        for _, txId := range txIds {
            cache.Pool.UnverifiedTxIndex.Put(txId.String(), payId)
        }
        cache.Pool.UnverifiedTxEntries.Put(payId.String(), entry)
      #+END_SRC

**** Transfer ("transfer")

     Incoming transfer command is processed by ~peer/process.go#215:
     processTransfer~. After some data checkings, store transfer record
     into pool by method ~reservoir/transfer.go#27: StoreTransfer~.

***** Verify transfer record

      At ~resovoir/transfer.go#127: verifyTransfer~.

      Check if any previous transaction exists.

      #+BEGIN_SRC go
        __, previousPacked := storage.Pool.Transactions.GetNB(newTransfer.GetLink().Bytes())
        if nil == previousPacked {
            return nil, false, fault.ErrLinkToInvalidOrUnconfirmedTransaction
        }

        previousTransaction, _, err := transactionrecord.Packed(previousPacked).Unpack(mode.IsTesting())
        if nil != err {
            return nil, false, err
        }
      #+END_SRC

      A new transfer record can happen on if previous transfer is in
      following types: issue, transfer, counter-signed transfer, old
      base data, block foundataion, block owner transfer

***** Generate transfer info

      A transfer info includes payment info, which is generated by
      previous transfer record.

      #+BEGIN_SRC go
        packedTransfer := verifyResult.packedTransfer
        payId := pay.NewPayId([][]byte{packedTransfer})

        txId := verifyResult.txId
        link := transfer.GetLink()
        if txId == link {
            // reject any transaction that links to itself
            // this should never occur, but protect against this situation
            return nil, false, fault.ErrTransactionLinksToSelf
        }

        previousTransfer := verifyResult.previousTransfer
        ownerData := verifyResult.ownerData

        payments := getPayments(ownerData, previousTransfer)

        result := &TransferInfo{
            Id:       payId,
            TxId:     txId,
            Packed:   packedTransfer,
            Payments: payments,
        }
      #+END_SRC

***** Already existed payment

      If a payment id is already existed in the cache pool, just
      return that payment info.

      #+BEGIN_SRC go
        if val, ok := cache.Pool.UnverifiedTxEntries.Get(payId.String()); ok {
            entry := val.(*unverifiedItem)
            if nil != entry.payments {
                result.Payments = entry.payments
            } else {
                // this would mean that reservoir data is corrupt
                logger.Panicf("StoreTransfer: failed to get current payment data for: %s  payid: %s", txId, payId)
            }
            return result, true, nil
        }
      #+END_SRC

      If duplicated transfer already exist but not found unverified
      record, return error.

      Remove payment record from ~OrphanPayment~ is already recieved.

***** Wait for payment info

      For a payment that has not verifyed, put record into cache pool.

      #+BEGIN_SRC go
        cache.Pool.PendingTransfer.Put(link.String(), txId)
        cache.Pool.UnverifiedTxIndex.Put(txId.String(), payId)
        cache.Pool.UnverifiedTxEntries.Put(
            payId.String(),
            &unverifiedItem{
                itemData: transferredItem,
                payments: payments,
            },
        )
      #+END_SRC

**** Proof ("proof")

     Incoming proof command is processed by ~peer/process.go#247:
     processProof~. After some data checkings, if the record is a velid
     proof block, return nil, otherwise return error.

     #+BEGIN_SRC go
       var payId pay.PayId
       nonceLength := len(packed) - len(payId) // could be negative
       if nonceLength < payment.MinimumNonceLength || nonceLength > payment.MaximumNonceLength {
           return fault.ErrInvalidNonce
       }
       copy(payId[:], packed[:len(payId)])
       nonce := packed[len(payId):]
       status := reservoir.TryProof(payId, nonce)
       if reservoir.TrackingAccepted != status {
           // pay id already processed or was invalid
           return fault.ErrPayIdAlreadyUsed
       }
       return nil
     #+END_SRC

**** RPC ("rpc")

     Incoming rpc command adds server to rpc list.

     #+BEGIN_SRC go
       if announce.AddRPC(arguments[0], arguments[1], timestamp) {
           messagebus.Bus.Broadcast.Send("rpc", arguments[0:3]...)
       }
     #+END_SRC

**** Peer ("peer")

     Incoming peer command adds server to peer list.

     #+BEGIN_SRC go
       if announce.AddPeer(arguments[0], arguments[1], timestamp) {
           messagebus.Bus.Broadcast.Send("peer", arguments[0:3]...)
       }
     #+END_SRC

**** Heart beat ("heart")

     Do nothing.

** Connector Peer Processing

   Connector behavior can be viewed as a finite state machine, the
   implementation is using for loop to execute.

   #+BEGIN_SRC go
     for conn.runStateMachine() {
     }
   #+END_SRC

   Each state is integer value defined at ~peer/connector.go#38~.

   #+BEGIN_SRC go
     const (
         cStateConnecting   connectorState = iota // register to nodes and make outgoing connections
         cStateHighestBlock connectorState = iota // locate node(s) with highest block number
         cStateForkDetect   connectorState = iota // read block hashes to check for possible fork
         cStateFetchBlocks  connectorState = iota // fetch blocks from current or fork point
         cStateRebuild      connectorState = iota // rebuild database from fork point (config setting to force total rebuild)
         cStateSampling     connectorState = iota // signal resync complete and sample nodes to see if out of sync occurs
     )
   #+END_SRC

   ~peer/connector.go#224: runStateMachine~. returns ~true~ or ~false~ used to
   decide if state machine should stop or not.

   Connector states are described as follows:

*** Conneting

    Check if every clint is connected and update client count. Stops
    loop if minimum clients are connected (3).

*** Hightest block

    Get highest block from all clients. Go to next state
    ~cStateForkDetect~ if there another node with larger block
    height. Otherwise, stop loop.

*** Fork detected

    If other node has larger block height, go to rebuild state.

    If current block height is larger than any other node's larget block
    height, find the latest common block. If current block height gets
    60 more blocks thatn latest common block, go to highest block
    state, otherwise, remove old blocks.

*** Fetch block

    Fetch new blocks from other nodes.

*** Rebuild

    Return to normal operation.

*** Sampling

    Check peers and block height to detect fork.
